# Corpus metrics

## data

`data/t.jsonl` is an excerpt from the WCEP dataset: <https://github.com/complementizer/wcep-mds-dataset>

Abstractiveness.py, Compression_score.py and IDS_Redundancy.py expect data in the following format:

```bash
├── data
│   ├── formatted
│   │   ├── 59849
│   │   │   ├── documents.txt
│   │   │   └── summary.txt
│   │   ├── 59850
│   │   │   ├── documents.txt
│   │   │   └── summary.txt
│   │   ├── 59851
│   │   │   ├── documents.txt
│   │   │   └── summary.txt
│   │   ├── 59852
```

where `data/formatted` contains a folder for each topic (with a unique name). Each topic folder contains 2 files:

- `data/formatted/[id]/documents.txt` => contains the documents, one per line with a blank line between document
- `data/formatted/[id]/summary.txt` => contains the reference summary for those documents

An example structure can be generated by running `node data/setup.js`, 
which takes the example data in `data/t.jsonl` and converts it into the required structure

Alternatively, optional arguments can be supplied: `node data/setup.js [inputFile] [outputFolder]`

- `inputFile` defaults to `data/t.jsonl`
- `outputFolder` defaults to `data/formatted`

## Dependencies

Dependencies are documented in `requirements.txt`.

Code is tested with Python 3.10.6

## Running the benchmark

After the data is ready and all dependencies are installed, simply run `python benchmark`.
This currently only evaluates Compression Score and Abstractness, as I did not yet figure out how to properly use the other metrics.